---
title: "Assignment_1"
format: html
editor: visual
---

```{r}
#| label: setup
#| echo: false
#| output: false
#| message: false
library(tidyverse)
library(readxl)
library(magrittr)
library(knitr)
library(dineq)
library(ineq)
library(ggplot2)
library(scales)
library(cowplot)
```

# Assignment 1: Data and Descriptive

This assignment aims to obtain, process, and analyze sub-national GDP and population data for a subset of European countries.
The selected subset comprises the Czech Republic, Turkey, Finland and the United Kingdom.
The objective is to calculate GDP per capita and explore regional inequity using a range of descriptive statistics and visualizations.

## Part A: Sub-national GDP and GDP per Capita

In the first part, we will dive into sub-national GDP for the selected European countries.
Firstly, a brief overview of the data acquisition will be given.
Furthermore, we will calculate GDP per capita for each sub-region.
Lastly, we will provide a descriptive analysis that gives insights into the distribution and the central tendencies of the data.

### Data Acquisition

The assignment is based on datasets providing sub-regional data on GDP and population from the year 2000 to 2020.
We had to obtain data for the UK in a different way than for the rest of the countries.
We will now take you through the data acquisition.

We obtained data for the EU countries (Czech Republic, Turkey and Finland) from Eurostat.
We downloaded two datasets, one providing GDP data (nama_10r_3gdp) and the other one providing population data (demo_r_pjanaggr3).
The NUTS classification is a hierarchical system that enables cross-border statistical comparisons at various regional levels within the EU.
The datasets we downloaded from Eurostat are at a NUTS 3 level.

We needed just a few of the variables included in the two datasets from Eurostat.
After importing the two datasets, we used the select()-function to pick the variables we wanted and gave them some more appropriate names.
We ended up with three variables in each dataset; "year", "geo", and "gdp"/"pop".

```{r}
# GDP data for Czech Republic, Turkey and Finland
gdp_data <- read_csv("nama_10r_3gdp__custom_7667248_linear.csv") |>
  filter(unit == "MIO_EUR") |>
  select(year = TIME_PERIOD, geo, gdp = OBS_VALUE)
```

```{r}
# Population data for Czech Republic, Turkey and Finland
population_data <- read_csv("demo_r_pjanaggr3__custom_7667383_linear.csv") |>
  select(year = TIME_PERIOD, geo, pop = OBS_VALUE)
```

The UK is not included in the datasets from Eurostat, since the country is no longer a part of the EU.
We got sub-regional GDP and population data for the UK from the Office for National Statistics.
The dataset we downloaded is called "Regional gross domestic product; all ITL regions".
The UK uses another classification system than the EU.
Instead of NUTS regions, they use ITL regions.
These two classification systems work on the same principle and we will from now on, refer to all regions as NUTS.

```{r}
# GDP data for UK
gdp_UK <- read_excel("regionalgrossdomesticproductgdpallitlregions.xlsx", 
    sheet = "Table 5", skip = 1)
```

```{r}
# Population data for UK
population_UK <- read_excel("regionalgrossdomesticproductgdpallitlregions.xlsx", 
    sheet = "Table 6", skip = 1)
```

To get the two UK datasets in the same format as the datasets from Eurostat, we used the gather()-function to convert the dataset from wide to long format.
With filter() and mutate() we made a "geo" variable showing regions at a NUTS 3 level.
We used the select()-function to pick the same variables we have in the datasets for the EU countries.

```{r}
# Reforming gdp_UK
gdp_UK <- gdp_UK %>%
  gather(year, gdp, -ITL, -`ITL code`, -`Region name`) %>%
  filter(ITL == "ITL3") %>%
  mutate(geo = `ITL code`) %>%
  select(year, geo, gdp) %>%
  arrange(geo)
```

```{r}
# Reforming population_UK
population_UK <- population_UK %>%
  gather(year, pop, -ITL, -`ITL code`, -`Region name`) %>%
  filter(ITL == "ITL3") %>%
  mutate(geo = `ITL code`) %>%
  select(year, geo, pop) %>%
  arrange(geo)
```

When sub-regional GDP and population data for both the EU countries and the UK were in place, we could start merging the datasets.
First, we merged the gdp dataset and the population dataset for the EU countries.
Then, we merged the two datasets for the UK.
When merging the UK datasets, we replaced "TL" with "UK" in the NUTS 3 code.
Finally, we merged the EU dataset with the UK dataset to get one dataset containing sub-regional GDP and population for all the countries.
We named the final dataset for "main_dataset".

```{r}
# Merging gdp dataset with population dataset for the EU countries
merged_data <- gdp_data |>
  left_join(population_data, join_by(year,geo))
```

```{r}
# Merging gdp dataset with population dataset for the UK
merged_UK <- gdp_UK |>
  left_join(population_UK, join_by(year,geo))
merged_UK$geo <- gsub("TL", "UK", merged_UK$geo)
```

```{r}
# Converting the "year" variable into a character
merged_data$year <- as.character(merged_data$year)
merged_UK$year <- as.character(merged_UK$year)
```

```{r}
# Merging EU dataset with UK dataset
main_data <- full_join(merged_data, merged_UK) 
```

```{r}
# We only want data from year 2000 to 2020
main_data <- main_data %>%
  subset(main_data$year >= 2000 & main_data$year <= 2020)
```

```{r}
# Clean up
rm(gdp_data, population_data, gdp_UK, population_UK, merged_data, merged_UK)
```

### GDP per Capita Calculation

Now that our datasets are joined, we can create a new variable for GDP per capita in our main dataset by dividing the "gdp" column by the "pop" column.
We do this by taking our main_data and using the mutate() - function, and the formula:

$y_{i} = \frac{GDP_{i}}{Population_{i}}$

```{r}
# Adding GDP per capita in the main_data
main_data <- main_data %>%
  mutate(gdp_per_capita = round(gdp / pop, digits = 4))
```

GDP per capita gives an average representation of the economic well-being of individuals in a country, indicating the amount of economic output generated, by each person.
it is often used as an indicator of the standard living and economic development within a country.

### Descriptive Analysis

Through descriptive statistic we will summarize and describe the main features and key characteristics of the dataset.
We use the summary() fnction to get a summary of each variable in the dataset.
The summary contains minimum value, median, mean, quartiles and maximum value, as well as numbers of NA\`s.

```{r}
summary(main_data)
```

We will take a closer look at minimum and maximum value, as well as the median and mean.
For the GDP variable, we have a big difference between the minimum (57.56) and maximum (242729.95) value.
This indicates great variation in the GDP among the regions.
We have both regions with very small GDP and regions with very high GDP.
We see that the mean is greater than the median (8820.94\>5979.00).
This suggest that there is some quite rich regions pulling the average up.
There is also a big difference between minimum (19220) and maximum (15067724) value for the population variable, which indicates great variation in the size of the regions.
The mean is also greater than the median for the population (493610\> 331108) .
Some big regions pulling the average up.
The variation in both GDP and population means that is also a great spread in the GDP per capita.
The minimum value is 0.00160 and the maximum value 0.41360, with a mean at 0.02130.
To summarize, there is great variation in the dataset.

We use standard deviation to measure the spread in GDP, population and GDP per capita.
The standard deviation is 13428.01 for the GDP variable and 830187 for the population variable.
The GDP per capita have a standard deviation of 0.02497901

```{r}
# standard deviation
# the column names for which we want to calculate the sd
columns_of_interest <- c("gdp", "pop", "gdp_per_capita")
# Store it in a empty vector
std_deviations <- numeric(length(columns_of_interest))
# Calculating the standard deviation, and removing "NA"
for (i in 1:length(columns_of_interest)) {
  col_name <- columns_of_interest[i]
  std_deviations[i] <- sd(main_data[[col_name]], na.rm = TRUE)
}
# Print the standard deviations
std_deviations
```

## Part B: Regional Inequity

The second part focuses on regional inequality.
We will start with a literate review of the subject, then compute population-weighted GDP Gini coefficients for the regions within our subset of European countries.
This includes data presentation through descriptive statistics and visuals.
In the end, we will discuss the computed Gini coefficients and reflect on the implications of our findings in the context of regional inequity.

### Literature Review

Regional inequality refers to the disparities in economic development, income, and other socio-economic indicators among different geographical regions within a country or across countries.

We should care about regional inequality because it is essential for sustainable and inclusive development in the future.
Regional inequality can have significant social, economic, and political implications.
It is an important topic that raises questions regarding the consequences and how we should deal with it.
Focusing on regional inequality is important for both the economy and social welfare in general.
Addressing regional inequality is important for ensuring that the benefits of economic growth are shared more equitably among all citizens.
It is all about creating a more balanced and fair society.

### Gini Coefficient Calculation

Measuring regional inequality is challenging because it is difficult to compare regions of different sizes.
The number of regions by each country also varies in our dataset.
To deal with this issue, we use a population-weighted Gini coefficient that accounts for different sizes of the regions.

To compute the population-weighted GDP Gini coefficient for each European NUTS 2 level region within the subset of countries, we used the formula below:

$GINW_j=\frac{1}{2\overline y j} \sum_{i}^{n_j} \sum_{l}^{n_j} \frac{p_i}{P_j} \frac{p_l}{P_j} |y_i - y_l|$

First, we created a new dataset (main_data_2) that includes three new variables with NUTS 0, NUTS 2 and NUTS 3 regions.
Then, we calculated the population-weighted GDP gini coefficient for each NUTS 2 level region.

```{r}
# Creating NUTS 2 and NUTS 0 level
main_data_2 <- main_data |>
  mutate(id_nuts3 = geo,
         id_nuts2 = substr(geo, 1,4),
         id_nuts0 = substr(geo, 1, 2)
         )
```

```{r}
# Calculating the population-weighted GDP Gini coefficient for each NUTS 2 level region
main_data_2 <- main_data_2 |>
  filter(pop != "NA") |>
  filter(pop != 0) |>
  filter(gdp_per_capita != "NA") |>
  group_by(id_nuts2, year) |>
  mutate(gini_n2 = round(gini.wtd(gdp_per_capita, weights = pop), digits = 4)) |>
  ungroup()
```

```{r}
# Cleaning the dataset
main_data_2 <- main_data_2 %>%
  select(year, id_nuts3, id_nuts2, id_nuts0, gdp, pop, gdp_per_capita, gini_n2, -geo)
```

Further, we created main_data_nuts2 that contains only the first row of each group defined by the combination of "id_nuts_2" and "year" from the dataset "main_data 2".

```{r}
main_data_nuts2 <- main_data_2 |>
 group_by(id_nuts2, year) |>
  slice(1)
```

### Data Presentation

```{r}
# Gini for all countries
gini_nuts0 <- main_data_nuts2 |>
  filter(!is.na(gini_n2)) |>
  group_by(id_nuts0) |>
  summarize(
    Mean = mean(gini_n2),
    Variance = var(gini_n2),
    Max = max(gini_n2),
    Min = min(gini_n2)
  )
```

```{r}
gini_nuts0 |>
  kable()
```

```{r}
# creating a scatterplot of the main_data to show how gdp per capita relates to the population and gdp in each region
ggplot(data = main_data, aes(x = gdp, y = pop, label = gdp_per_capita)
       ) +
  geom_point(size = 0.1) +
  labs(x = "gdp", y = "pop", title = "scatterplot of gdp and pop") +
  scale_color_brewer(palette = "Set1") +
  theme_minimal()
```

```{r}
# creating a scatterplot of the main_data to show how gdp per capita relates to the population and gdp in each region
ggplot(data = main_data, aes(x = gdp, y = pop, label = gdp_per_capita)
       ) +
  geom_point(size = 0.1) +
  labs(x = "gdp", y = "pop", title = "scatterplot of gdp and pop") +
  scale_color_brewer(palette = "Set1") +
  theme_minimal()
```

Subset 2018 from main data:

```{r}
subset_nuts2data <- main_data_nuts2 %>%
  filter(grepl("2018", year))
```

```{r}
# Create the scatterplot
scatter_plot <- ggplot(subset_nuts2data, aes(x = pop, y = seq(1:80), color = id_nuts2)) +
  geom_point() +
  labs(x = "Population", y = "NUTS2 Region")
plot_grid(scatter_plot)
```

Without outlier from Turkie\

```{r}
regression_model <- lm(seq(1:80) ~ pop, data = subset_nuts2data)

# Create the scatterplot
scatter_plot <- ggplot(subset_nuts2data, aes(x = pop, y = seq(1:80), color = id_nuts2)) +
  geom_point() +
  geom_abline(intercept = coef(regression_model)[1], slope = coef(regression_model)[2]) +
  labs(x = "Population", y = "NUTS2 Region") +
  scale_x_continuous(limits = c(0, 6000000))
plot_grid(scatter_plot)
```

```{r}
summary(regression_model)
```

Subset UK from nuts2:

```{r}
subset_nuts2data_UK <- main_data_nuts2 %>%
  filter(grepl("^UK", id_nuts2) & year == 2018)
```

```{r}

regression_model_UK <- lm(seq(1:41) ~ pop, data = subset_nuts2data_UK)
# Create the scatterplot
scatter_plot <- ggplot(subset_nuts2data_UK, aes(x = pop, y = seq(1:41), color = id_nuts2)) +
  geom_point() +
  geom_abline(intercept = coef(regression_model_UK)[1], slope = coef(regression_model_UK)[2]) +
  labs(x = "Population", y = "NUTS2 Region")
plot_grid(scatter_plot)
```

```{r}
summary(regression_model_UK)
```

1.  **Residuals**: This section provides statistics about the residuals, which are the differences between the observed values and the values predicted by the model.
    It includes the minimum, 1st quartile (1Q), median, 3rd quartile (3Q), and maximum values of the residuals.

2.  **Coefficients**: This section displays information about the coefficients of the linear regression model:

    -   **Estimate**: This is the estimated value of the intercept and the slope.
        In your case, the estimated intercept is approximately 23 (2.3e+01) and the estimated slope is approximately -5.436e-06.

    -   **Std. Error**: This represents the standard errors of the coefficients.

    -   **t value**: The t-value measures the number of standard errors the estimated coefficient is away from zero.
        It is used to test the significance of the coefficient.

    -   **Pr(\>\|t\|)**: This is the p-value associated with the t-statistic.
        It indicates the probability of observing a t-statistic as extreme as the one computed if the true coefficient were zero.

    -   **Signif. codes**: These symbols (**, *, .) indicate the level of significance of the coefficients. In this case, '***' indicates high significance for the intercept, while ' ' (a space) suggests that the 'pop' coefficient is not statistically significant.

3.  **Residual standard error**: This is the standard deviation of the residuals, which measures the spread of the residuals around the regression line.

4.  **Multiple R-squared**: This is a measure of the goodness of fit.
    It tells you how well the independent variable(s) explain the variation in the dependent variable.
    In your case, the adjusted R-squared is very close to zero, which means that 'pop' explains very little of the variation in 'seq(1:41)'.

5.  **F-statistic**: This is a test for overall significance of the model.
    It assesses whether your model, with all its coefficients, is better than a model with no predictors.
    In this case, the F-statistic is relatively low, indicating that the model as a whole may not be significant.

6.  **p-value**: This is the p-value associated with the F-statistic.
    In your output, it is relatively high (0.5126), indicating that the model as a whole may not be statistically significant.

In summary, the regression model doesn't seem to provide a strong fit for the data, and the 'pop' variable does not appear to be a significant predictor of 'seq(1:41)' based on the p-values and the low adjusted R-squared value.

### Discussion
